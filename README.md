<div align="center">

<br />
<img src="public/og-image.svg" alt="Token Counter" width="420" />

# ğŸ”¢ Tokenizer â€” Universal AI Token Counter

<p>
<strong>Fast, accurate, 100% clientâ€‘side token counting</strong> for GPT, Llama, Mistral, Qwen, DeepSeek, Phi, and more â€” all in your browser. No API keys. No backend. Just drop text or files and get counts, context usage, and optional cost estimates instantly.
</p>

[![CI](https://img.shields.io/github/actions/workflow/status/Nepomuceno/tokenizer/ci.yml?label=CI&logo=github)](../../actions)
[![Deploy](https://img.shields.io/github/actions/workflow/status/Nepomuceno/tokenizer/pages.yml?label=Pages&logo=github)](https://nepomuceno.github.io/tokenizer/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Made with React](https://img.shields.io/badge/React-19-61dafb?logo=react&logoColor=white)](https://react.dev)
[![Bun](https://img.shields.io/badge/Bun-powered-000?logo=bun&logoColor=white)](https://bun.sh)
[![Buy Me A Coffee](https://img.shields.io/badge/â˜•ï¸-Buy%20me%20a%20coffee-orange)](#-support)

ğŸ”— <strong>Live Demo:</strong> https://nepomuceno.github.io/tokenizer/

<sub>Deploys automatically on push to <code>main</code>.</sub>

</div>

---

## âœ¨ Features

| Area | Highlights |
|------|------------|
| Tokenization | Supports GPT (tiktoken), Llama, Mistral, Qwen, DeepSeek, Phi via HuggingFace tokenizers |
| File Ingestion | PDF, Markdown, TXT, DOCX, CSV â€” fully clientâ€‘side extraction |
| Performance | Web Worker offload + lazy WASM loading; handles large files in slices |
| Context Awareness | Shows model context limit + progress bar + overâ€‘limit warning |
| Cost Estimation | Editable perâ€‘1K price; totals by file and aggregate |
| Batch Mode | Drag & drop multiple files with perâ€‘file + total counts |
| Token Preview | Colorâ€‘coded token splits for insight & debugging |
| Offlineâ€‘Ready | Static build suitable for GitHub Pages; no server dependency |
| UX & Accessibility | Keyboard friendly, ARIA labels, dark mode planned |

---

## ğŸ§  How It Works

1. User pastes text or drops files.
2. Parsers extract plain text (PDF via `pdfjs-dist`, DOCX via `mammoth`, Markdown via `remark`, raw readers for TXT/CSV).
3. Text is chunked (100â€“500KiB) and sent to a Web Worker.
4. The worker loads the appropriate tokenizer adapter:
   * `@dqbd/tiktoken` for OpenAIâ€‘style models (WASM)
   * `@huggingface/tokenizers` JSON for Llama/Mistral/Qwen/Phi/DeepSeek
5. The adapter counts tokens and streams progress back.
6. UI displays perâ€‘file stats, model context usage, optional price math, and token preview.

All processing stays in the browser. Nothing is uploaded.

---

## ğŸ—‚ Model Registry

Models are defined centrally in `src/modelRegistry.ts` with: key, adapter type, resource (internal tokenizer name or JSON path), context window, and optional pricing reference. This keeps logic declarative and avoids hardâ€‘coding throughout the UI.

---

## ğŸš€ Quick Start (Local)

Prereqs: Bun (https://bun.sh) installed.

```bash
git clone https://github.com/Nepomuceno/tokenizer.git
cd tokenizer
bun install
bun run dev
```

Build:
```bash
bun run build
```

Preview production build:
```bash
bun run preview
```

Run tests:
```bash
bun run test
```

Lint:
```bash
bun run lint
```

---

## ğŸ§ª Testing Philosophy

Minimal, focused Vitest tests cover tokenizer adapters and integration boundaries. When adding a new model or parser:

1. Add adapter tests (`src/test/*`).
2. Add a small fixture (avoid large binaries in repo).
3. Ensure deterministic count vs. known sample.

---

## ğŸ“ Architecture Overview

```
React UI (components)  --->  Tokenizer Factory  --->  Adapter (tiktoken | hf)  --->  WASM / JSON tokenizer data
        |                        ^                        
        |                        |                        
File Parsers (pdf/markdown/docx/text) ----> Web Worker (tokenize.worker.ts) <---- Model Registry (ctx + meta)
```

Key principles:
* Keep UI pure & declarative.
* Lazy load heavyweight assets (WASM / tokenizer JSON).
* Use transferable data (ArrayBuffer) when scaling.
* Avoid blocking the main thread.

---

## ğŸ›  Tech Stack

| Category | Tooling |
|----------|---------|
| Framework | React + TypeScript + Vite |
| Runtime / PM | Bun |
| Styling | Tailwind CSS |
| Tokenizers | `@dqbd/tiktoken`, HuggingFace tokenizer JSONs |
| Parsing | `pdfjs-dist`, `remark` + `remark-parse`, `mammoth` |
| Testing | Vitest + Testing Library |
| Deployment | GitHub Pages via Actions |

---

## ğŸ§© Adding a New Model

1. Drop tokenizer JSON in `public/tokenizers/` (if HF style).
2. Add entry to `modelRegistry.ts` with key + ctx size.
3. (Optional) Add pricing metadata.
4. Write/extend adapter tests.
5. Build & verify counts.

---

## ğŸ¤ Contributing

## ğŸ“¦ Deployment Notes (GitHub Pages)

The app is deployed under a subpath (`https://<user>.github.io/tokenizer/`). To keep static assets resolving correctly:

* `vite.config.ts` uses `base: './'` so built asset references are relative.
* All favicon / manifest links in `index.html` use `./` prefixes (no leading `/`).
* `public/manifest.json` sets `icons[].src`, `start_url`, and `scope` to `./` forms.
* The tiktoken adapter builds its local encodings path from `import.meta.env.BASE_URL` so it fetches `./encodings/*.tiktoken` instead of hitting the domain root (`/encodings`), avoiding 404 + CORS when hosted in a subfolder.
* Avoid introducing absolute leading-slash asset paths unless you change deployment to the domain root.

If you fork and deploy at another subpath, no changes are needed; if you deploy at root, everything still works because relative paths resolve there as well.


Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed setup, guidelines, and PR checklist.

Please follow conventional commits where practical (e.g., `feat: add qwen tokenizer`).

---

## ğŸ” Security

Report vulnerabilities privately via GitHub Security Advisories or the email listed in [SECURITY.md](SECURITY.md). Avoid opening public issues for sensitive disclosures.

---

## â˜•ï¸ Support

If this project saves you time, you can buy me a coffee:

https://buymeacoffee.com/gabrielbici

Sharing the repo also helps a ton! ğŸ’™

---

## ğŸ“œ License

Licensed under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgements

Inspired by community token counters and upstream tokenizer libraries. Thanks to:
* `@dqbd/tiktoken`
* HuggingFace `tokenizers`
* Openâ€‘source parser ecosystems (pdfjs, remark, mammoth)

---

## ğŸ—º Roadmap (Highâ€‘Level)

- [ ] Dark mode toggle
- [ ] PWA / offline caching
- [ ] Custom userâ€‘uploaded tokenizer JSON
- [ ] Cost presets per vendor
- [ ] CSV Export of batch results

<sub>Have an idea? Open a discussion or PR!</sub>

---

### Star History

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://starchart.cc/Nepomuceno/tokenizer.svg?theme=dark" />
  <img alt="Star History Chart" src="https://starchart.cc/Nepomuceno/tokenizer.svg" />
</picture>

---

<div align="center">
<strong>Built with â¤ï¸ for the AI developer community.</strong>
</div>

